{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a5c71e7-dc30-4cfa-a4bc-615ad6c537e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "from collections import deque\n",
    "\n",
    "# deep learning imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83f6933b-14dd-46b2-904c-c41f7beda298",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./../../.local/bin/dc-grocery-outlet-models/image_splits_10_or_less\"\n",
    "\n",
    "\n",
    "def list_files(d):\n",
    "    \"\"\"\n",
    "        just lists files given a directory (str) d\n",
    "    \"\"\"\n",
    "    r = []\n",
    "    for root, dirs, files in os.walk(d):\n",
    "        for name in files:\n",
    "            r.append(os.path.join(root, name))\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f8d19c2-474b-42dc-babb-2ea972b0e2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 50 files\n"
     ]
    }
   ],
   "source": [
    "files = list_files(directory)\n",
    "print(f\"there are {len(files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e7b2ab6-ddd4-4d3f-bec7-7079733a844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_make_image_splits(file):\n",
    "    \"\"\"\n",
    "        given a .pkl file, returns a 3D numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    pkl_file = open(file, 'rb')\n",
    "\n",
    "    data1 = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "\n",
    "    return data1\n",
    "\n",
    "def preprocess(file):\n",
    "    \"\"\"\n",
    "        given a file, returns generators for training, test, and validation\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(read_make_image_splits(file), columns=[\"brand\", \"size\", \"measure\", \"class\", \"long_description\", \"ImageList\", \"image_matrix\", \"image_tensor\"])\n",
    "    df = df.dropna()\n",
    "    df_brand = df[[\"image_tensor\", \"brand\"]]\n",
    "    \n",
    "    # to be replaced\n",
    "    train = list(map(list, zip(df_brand[\"image_tensor\"].values, df_brand[\"brand\"].values)))\n",
    "    \n",
    "    random.shuffle(train)\n",
    "    val_split = int(len(train)*0.8)\n",
    "    test_split = int(len(train)*0.9)\n",
    "    val = train[val_split:test_split]\n",
    "    test = train[test_split:]\n",
    "    train = train[:val_split]\n",
    "    # print(train[0])\n",
    "    # print(train[0][0])\n",
    "    # train, val = train_test_split(train, test_size=0.2)\n",
    "    # train, test = train_test_split(train, test_size=0.1)\n",
    "\n",
    "    train_generator = data.DataLoader(train, batch_size=16)\n",
    "    test_generator = data.DataLoader(test, batch_size=16)\n",
    "    val_generator = data.DataLoader(val, batch_size=16)\n",
    "    \n",
    "    return train_generator, test_generator, val_generator, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "364b9015-b78b-479c-b448-67f8ab39475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = pd.DataFrame(read_make_image_splits(files[0]), \n",
    "                  columns=[\"brand\", \"size\", \"measure\", \"class\", \"long_description\", \"ImageList\", \"image_matrix\", \"image_tensor\"])\n",
    "f1 = f1.dropna()\n",
    "f2 = pd.DataFrame(read_make_image_splits(files[1]), \n",
    "                  columns=[\"brand\", \"size\", \"measure\", \"class\", \"long_description\", \"ImageList\", \"image_matrix\", \"image_tensor\"])\n",
    "f2 = f2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "009e728d-06ff-4700-baa9-a093ebb070cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "brand                                       made good                \n",
       "size                                                                6\n",
       "measure                                                       Count  \n",
       "class                                                              56\n",
       "long_description             MADE GOOD ORGANIC CRISPY SQUARES CARAMEL\n",
       "ImageList                                     prod/00687456213415.jpg\n",
       "image_matrix        [[[255, 255, 255], [255, 255, 255], [255, 255,...\n",
       "image_tensor        [[[tensor(255, dtype=torch.uint8), tensor(255,...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25ccb023-136b-47b6-a74e-4a8e3e9e29dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1brands = set(f1['brand'].unique())\n",
    "#f2brands = set(f2['brand'].unique())\n",
    "#len(list(f1brands - f2brands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "913b8d9d-cfd9-4a74-8150-c90063a91bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.sum(f1.brand.isin(f2brands)) / len(f1brands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddae78ec-7bbb-45d0-b07c-2d0c26cd17de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroceryDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data, transform=None):\n",
    "        super(GroceryDataset, self).__init__()\n",
    "        np.random.seed(42)\n",
    "        self.transform = transform\n",
    "        self.data = data # f1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # image1 = random.choice(self.dataset.imgs)\n",
    "        '''\n",
    "            Gets two images from dataframe from either the same or different classes randomly \n",
    "            with the appropriate label (0 = different brand, 1 = same brand)\n",
    "        '''\n",
    "        x = np.random.uniform()\n",
    "        sample = self.data.sample(1)\n",
    "        image1 = sample['image_tensor'].iloc[0]\n",
    "        brand1 = sample['brand'].iloc[0]\n",
    "        if (x >= 0.9):\n",
    "            sample2 = self.data[self.data['brand'] == brand1].sample(1)\n",
    "            label = 1\n",
    "        else:\n",
    "            sample2 = self.data[self.data['brand'] != brand1].sample(1)\n",
    "            label = 0\n",
    "        image2 = sample2['image_tensor'].iloc[0]\n",
    "        brand2 = sample2['brand'].iloc[0]\n",
    "        if self.transform:\n",
    "            image1 = self.transform(image1)\n",
    "            image2 = self.transform(image2)\n",
    "        return image1, image2, torch.from_numpy(np.array([label], dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0477293c-9c15-4157-8cbb-3d10fcf3d3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siamese(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Siamese, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 10),  # 64@96*96\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 64@48*48\n",
    "            nn.Conv2d(64, 64, 7),\n",
    "            nn.ReLU(),    # 128@42*42\n",
    "            nn.MaxPool2d(2),   # 128@21*21\n",
    "            nn.Conv2d(64, 32, 4),\n",
    "            nn.ReLU(), # 128@18*18\n",
    "            nn.MaxPool2d(2), # 128@9*9\n",
    "            nn.Conv2d(32, 32, 4),\n",
    "            nn.ReLU(),   # 256@6*6\n",
    "        )\n",
    "        self.linear = nn.Sequential(nn.Linear(9216, 4096), nn.Sigmoid())\n",
    "        self.out = nn.Linear(4096, 1)\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        out1 = self.forward_one(x1)\n",
    "        out2 = self.forward_one(x2)\n",
    "        dis = torch.abs(out1 - out2)\n",
    "        out = self.out(dis)\n",
    "        #  return self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30a80262-d955-4349-b9a7-acd6c86f4f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([transforms.Resize((224, 224))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99724d52-e159-4b59-9109-28729d940fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flags = {\"batch_size\" : 128, \"lr\" : 0.00006, \"show_every\" : 10, \"save_every\" : 100, \"test_every\" : 100, \"max_iter\" : 50000, \"model_path\" : \"./siamese\",\n",
    "         \"gpu_ids\" : \"0, 1, 2, 3\", \"workers\" : 4, \"times\" : 400, \"way\" : 20, \"cuda\" : True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d5dd38b-766a-4bc2-8a3f-e828fc91bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss(size_average=True)\n",
    "net = Siamese()\n",
    "net.cuda()\n",
    "net.train()\n",
    "\n",
    "trainSet = GroceryDataset(f1, transform=data_transforms)\n",
    "valSet = GroceryDataset(f2, transform=data_transforms)\n",
    "\n",
    "trainLoader = DataLoader(trainSet, batch_size=Flags['way'], shuffle=False, num_workers=2)\n",
    "valLoader = DataLoader(valSet, batch_size=Flags['batch_size'], shuffle=False, num_workers=2)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = Flags['lr'] )\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d99fd8d-660b-4b21-8cad-dbb1808258c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 15.74 GiB total capacity; 395.06 MiB already allocated; 15.75 MiB free; 396.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m img1, img2, label \u001b[38;5;241m=\u001b[39m img1\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mFloatTensor)\u001b[38;5;241m.\u001b[39mcuda(), img2\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mFloatTensor)\u001b[38;5;241m.\u001b[39mcuda(), label\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, label)\n\u001b[1;32m     15\u001b[0m loss_val \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mSiamese.forward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x1, x2):\n\u001b[0;32m---> 28\u001b[0m     out1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     out2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_one(x2)\n\u001b[1;32m     30\u001b[0m     dis \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(out1 \u001b[38;5;241m-\u001b[39m out2)\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mSiamese.forward_one\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_one\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     24\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/pooling.py:162\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_jit_internal.py:422\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:719\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    718\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 719\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 15.74 GiB total capacity; 395.06 MiB already allocated; 15.75 MiB free; 396.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "loss_val = 0\n",
    "time_start = time.time()\n",
    "queue = deque(maxlen=20)\n",
    "\n",
    "# input parameters to the model\n",
    "\n",
    "for batch_id, (img1, img2, label) in enumerate(trainLoader, 1): # this is just one-indexing\n",
    "    if batch_id > Flags['max_iter']:\n",
    "        break\n",
    "    img1, img2, label = img1.type(torch.FloatTensor).cuda(), img2.type(torch.FloatTensor).cuda(), label.cuda()\n",
    "    optimizer.zero_grad()\n",
    "    output = net.forward(img1, img2)\n",
    "    loss = loss_fn(output, label)\n",
    "    loss_val += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_id % Flags['show_every'] == 0 :\n",
    "        print('[%d]\\tloss:\\t%.5f\\ttime lapsed:\\t%.2f s'%(batch_id, loss_val/Flags['show_every'], time.time() - time_start))\n",
    "        loss_val = 0\n",
    "        time_start = time.time()\n",
    "    if batch_id % Flags['save_every'] == 0:\n",
    "        torch.save(net.state_dict(), Flags['model_path'] + '/model-inter-' + str(batch_id+1) + \".pt\")\n",
    "    if batch_id % Flags['test_every'] == 0:\n",
    "        right, error = 0, 0\n",
    "        for _, (test1, test2) in enumerate(testLoader, 1):\n",
    "            if Flags['cuda']:\n",
    "                test1, test2 = test1.cuda(), test2.cuda()\n",
    "            test1, test2 = Variable(test1), Variable(test2)\n",
    "            output = net.forward(test1, test2).data.cpu().numpy()\n",
    "            pred = np.argmax(output)\n",
    "            if pred == 0:\n",
    "                right += 1\n",
    "            else: error += 1\n",
    "        print('*'*70)\n",
    "        print('[%d]\\tTest set\\tcorrect:\\t%d\\terror:\\t%d\\tprecision:\\t%f'%(batch_id, right, error, right*1.0/(right+error)))\n",
    "        print('*'*70)\n",
    "        queue.append(right*1.0/(right+error))\n",
    "    train_loss.append(loss_val)\n",
    "#  learning_rate = learning_rate * 0.95\n",
    "\n",
    "with open('train_loss', 'wb') as f:\n",
    "    pickle.dump(train_loss, f)\n",
    "\n",
    "acc = 0.0\n",
    "for d in queue:\n",
    "    acc += d\n",
    "print(\"#\"*70)\n",
    "print(\"final accuracy: \", acc/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca4c06e-a168-4d91-af3d-97306a1de34d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bc8f40-bb4b-489d-988a-4e5cf50df04f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4b49d2-81ed-492a-99ea-f7ce2fadf050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f232a2b0-772a-42f3-ad92-b50aaafcf624",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
