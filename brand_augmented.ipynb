{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instructions: run entire notebook after specifying directory variable at the top\n",
    "> model = resnet152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/home/paperspace/Documents/dc-grocery-outlet-brand/image_splits_v3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'brand_model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils import data\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import random\n",
    "from model import *\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from PIL import Image\n",
    "\n",
    "import pprint, pickle\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(d):\n",
    "    r = []\n",
    "    for root, dirs, files in os.walk(d):\n",
    "        for name in files:\n",
    "            r.append(os.path.join(root, name))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "4bd4e024-b1ca-4eb7-adf6-4eb8f9110816",
     "kernelId": "4f378984-b18d-4458-aec5-11036b1449fe"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "2369\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [66]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(get_out_features())\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(num_ftrs, \u001b[38;5;241m16358\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:899\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    896\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 899\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 570\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    574\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    575\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    581\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:593\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 593\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:897\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    896\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 897\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# model = models.resnet34(pretrained=True)\n",
    "model = models.resnet152(pretrained=True) # we do not specify pretrained=True, i.e. do not load default weights\n",
    "# model.load_state_dict(torch.load('XNAS/trained_models/xnas_small_cifar10.t7'), strict=False)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "num_ftrs = model.fc.in_features\n",
    "print(get_out_features())\n",
    "model.fc = nn.Linear(num_ftrs, 16358)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('UPC-Images-Sample-Labeled.csv').drop(['Short Description', 'ItemNumber', 'SubClass', 'Department', 'class_subclass', 'label'], axis = 1)\n",
    "# df.Brand = df.Brand.str.lower()\n",
    "# df = df.sort_values(by='Brand', ascending=True)\n",
    "# no_dupes_df = df.drop_duplicates(\"ImageList\")\n",
    "# brand_df = no_dupes_df[no_dupes_df.Brand.notna()]\n",
    "# # brands_10_or_less = brand_df.groupby('Brand').filter(lambda subdf: len(subdf) <= 10)['Brand']\n",
    "\n",
    "# # more_than_10_df = brand_df[~brand_df['Brand'].isin(brands_10_or_less)]\n",
    "# brand_df.Brand = brand_df.Brand.str.lower()\n",
    "# brand_df.Brand = branddf.Brand.str.replace(' ', '')\n",
    "\n",
    "# brand_to_num = dict(zip(np.unique(more_than_10_df['Brand'].values), np.arange(len(np.unique(more_than_10_df['Brand'].values)))))\n",
    "# num_to_brand = dict(zip(np.arange(len(np.unique(more_than_10_df['Brand'].values))), np.unique(more_than_10_df['Brand'].values)))\n",
    "# brand_to_num.update(num_to_brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_make_image_splits(file):\n",
    "    \"\"\"\n",
    "        given a .pkl file, returns a 3D numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    pkl_file = open(file, 'rb')\n",
    "\n",
    "    data1 = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "\n",
    "    return data1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "64b15a06-9d16-4d68-a130-7c6c2d4298ab",
     "kernelId": "4f378984-b18d-4458-aec5-11036b1449fe"
    }
   },
   "outputs": [],
   "source": [
    "def test_model(model, generator):\n",
    "    pbar = tqdm(val_generator)\n",
    "    running_corrects = 0\n",
    "    for step, batch in enumerate(pbar):\n",
    "        model.eval()\n",
    "      # inputs = inputs.to(device)\n",
    "      # labels = labels.to(device)\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(torch.float)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "      # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "      # forward\n",
    "      # track history if only in train\n",
    "      # with torch.set_grad_enabled(phase == 'train'):\n",
    "      \n",
    "    \n",
    "      \n",
    "      #print(tuple(images)[0])\n",
    "      # print(labels)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            # print(preds)\n",
    "      \n",
    "    epoch_acc = running_corrects / len(val)\n",
    "    return epoch_acc.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "5a091f1f-ae6c-4be7-bccc-ed41441e1497",
     "kernelId": "4f378984-b18d-4458-aec5-11036b1449fe"
    }
   },
   "source": [
    "## train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToTensor()"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, save_path, epoch):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch' : epoch\n",
    "    }, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, load_path):\n",
    "    checkpoint = torch.load(load_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    # epoch = checkpoint['epoch']\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"/home/paperspace/Documents/dc-grocery-outlet-brand/image_splits_v3/train\"\n",
    "val_dir = \"/home/paperspace/Documents/dc-grocery-outlet-brand/image_splits_v3/val\"\n",
    "test_dir = \"/home/paperspace/Documents/dc-grocery-outlet-brand/image_splits_v3/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "cb034852-b0b2-44f4-b06c-fa4521d4ac33",
     "kernelId": "4f378984-b18d-4458-aec5-11036b1449fe"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "best_acc = 0\n",
    "losses = []\n",
    "scores = []\n",
    "best_model_wts = None\n",
    "model_path = 'brand_model.pt'\n",
    "start = time.time()\n",
    "patience = 10\n",
    "trigger_times = 0\n",
    "flag = False\n",
    "train_files = list_files(train_dir)\n",
    "test_files = list_files(test_dir)\n",
    "val_files = list_files(val_dir)\n",
    "og_files = list_files(\"/home/paperspace/Documents/dc-grocery-outlet-brand/image_splits_v2\")\n",
    "og_files_2 = list_files(\"/home/paperspace/Documents/dc-grocery-outlet-brand/image_splits_10_or_less\")\n",
    "files = list(zip(train_files, test_files, val_files, og_files, og_files_2))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
    "# model, optimizer = load_checkpoint(model, optimizer, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = []\n",
    "# for file in og_files:\n",
    "#     df = pd.DataFrame(read_make_image_splits(file), columns=[\"brand\", \"size\", \"measure\", \"class\", \"long_description\", \"ImageList\", \"image_matrix\", \"image_tensor\"])\n",
    "#     df = df.dropna()\n",
    "#     indices.append(random.shuffle(list(range(len(df)))))\n",
    "\n",
    "# less_than_10_indices = []\n",
    "# for file in og_files_2:\n",
    "#     df = pd.DataFrame(read_make_image_splits(file), columns=[\"brand\", \"size\", \"measure\", \"class\", \"long_description\", \"ImageList\", \"image_matrix\", \"image_tensor\"])\n",
    "#     df = df.dropna()\n",
    "#     less_than_10_indices.append(random.shuffle(list(range(len(df)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file):\n",
    "    \"\"\"\n",
    "        given a file, returns generators for training\n",
    "    \"\"\"\n",
    "\n",
    "    train_df = pd.DataFrame(read_make_image_splits(file[0]), columns=[\"brand\", \"size\", \"measure\", \"class\", \"long_description\", \"ImageList\", \"image_matrix\", \"image_tensor\"])\n",
    "    train_df = train_df.dropna()\n",
    "    train_df_brand = train_df[[\"image_tensor\", \"brand\"]]\n",
    "   \n",
    "    train = list(map(list, zip(train_df_brand[\"image_tensor\"].values, train_df_brand[\"brand\"].values))) \n",
    "    \n",
    "    train_generator = data.DataLoader(train, batch_size=16)\n",
    "    \n",
    "    test_df = pd.DataFrame(read_make_image_splits(file[1]), columns=[\"brand\", \"size\", \"measure\", \"class\", \"long_description\", \"ImageList\", \"image_matrix\", \"image_tensor\"])\n",
    "    test_df = test_df.dropna()\n",
    "    test_df_brand = test_df[[\"image_tensor\", \"brand\"]]\n",
    "   \n",
    "    test = list(map(list, zip(test_df_brand[\"image_tensor\"].values, test_df_brand[\"brand\"].values)))\n",
    "    \n",
    "    test_generator = data.DataLoader(test, batch_size=16)\n",
    "    \n",
    "    val_df = pd.DataFrame(read_make_image_splits(file[2]), columns=[\"brand\", \"size\", \"measure\", \"class\", \"long_description\", \"ImageList\", \"image_matrix\", \"image_tensor\"])\n",
    "    val_df = val_df.dropna()\n",
    "    val_df_brand = val_df[[\"image_tensor\", \"brand\"]]\n",
    "   \n",
    "    val = list(map(list, zip(val_df_brand[\"image_tensor\"].values, val_df_brand[\"brand\"].values)))\n",
    "\n",
    "    \n",
    "    val_generator = data.DataLoader(val, batch_size=16)\n",
    "    \n",
    "    return train_generator, test_generator, val_generator, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "----------\n",
      "File 0/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/133 [00:00<?, ?it/s]/pytorch/aten/src/ATen/native/cuda/Loss.cu:247: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:247: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:247: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:247: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:247: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:247: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:247: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:247: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [10,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:247: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:247: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:247: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:247: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:247: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "  0%|                                                   | 0/133 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of dims dont match in permute\n",
      "File 0/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/133 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of dims dont match in permute\n",
      "File 0/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/133 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of dims dont match in permute\n",
      "File 0/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/133 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of dims dont match in permute\n",
      "File 0/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/133 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of dims dont match in permute\n",
      "File 0/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/133 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of dims dont match in permute\n",
      "File 0/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/133 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of dims dont match in permute\n",
      "File 0/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/133 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of dims dont match in permute\n",
      "File 0/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/133 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of dims dont match in permute\n",
      "File 0/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/133 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of dims dont match in permute\n",
      "File 0/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/133 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of dims dont match in permute\n",
      "File 0/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/133 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of dims dont match in permute\n",
      "File 0/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/133 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of dims dont match in permute\n",
      "File 0/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/133 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of dims dont match in permute\n",
      "File 0/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/133 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of dims dont match in permute\n",
      "File 0/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/133 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of dims dont match in permute\n",
      "File 0/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/133 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of dims dont match in permute\n",
      "File 0/50\n",
      "----------\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [65]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     train_generator, test_generator, val_generator, val \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m tqdm(train_generator)\n\u001b[1;32m     18\u001b[0m     running_avg_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Input \u001b[0;32mIn [64]\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     10\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mzip\u001b[39m(train_df_brand[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues, train_df_brand[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrand\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues))) \n\u001b[1;32m     12\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mDataLoader(train, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mread_make_image_splits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrand\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeasure\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong_description\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImageList\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     15\u001b[0m test_df \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m     16\u001b[0m test_df_brand \u001b[38;5;241m=\u001b[39m test_df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrand\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36mread_make_image_splits\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    given a .pkl file, returns a 3D numpy array\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m pkl_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m data1 \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkl_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m pkl_file\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data1\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "    print('-' * 10)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
    "    # model, optimizer = load_checkpoint(model, optimizer, model_path)\n",
    "    file_num = 0\n",
    "    decrease = False\n",
    "    \n",
    "    for file in files:\n",
    "        print('File {}/{}'.format(file_num, len(files)))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        try:\n",
    "            train_generator, test_generator, val_generator, val = preprocess(file)\n",
    "\n",
    "            pbar = tqdm(train_generator)\n",
    "            running_avg_loss = 0\n",
    "            for step, batch in enumerate(pbar):\n",
    "                model.train()\n",
    "                inputs, labels = batch\n",
    "                inputs = inputs.to(torch.float)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                # with torch.set_grad_enabled(phase == 'train'):\n",
    "                '''images = []\n",
    "                for image_path in inputs:\n",
    "                    img = Image.open(os.path.join(image_dir, image_path))\n",
    "                    images.append(transforms.ToTensor()(img))'''\n",
    "\n",
    "                #print(tuple(images)[0])\n",
    "                # print(labels)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # _, preds = torch.max(outputs, 1)\n",
    "                # print(labels)\n",
    "                # print(outputs)\n",
    "                # print(outputs.shape)\n",
    "                loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "                # print(loss)\n",
    "                # backward + optimize only if in training phase\n",
    "                loss.backward()\n",
    "                # print(loss)\n",
    "                # pbar.set_description(str(loss.item()))\n",
    "                optimizer.step()\n",
    "                # avg_loss = (avg_loss + loss.item()) / (step + 1)\n",
    "                running_avg_loss += loss.item()\n",
    "                pbar.set_description(\"Avg Loss: \" + str(running_avg_loss / (step + 1)))\n",
    "\n",
    "                # statistics\n",
    "                # running_loss += loss.item() * inputs.size(0)\n",
    "                # running_corrects += torch.sum(preds == labels.data)\n",
    "                # scheduler.step()\n",
    "\n",
    "                 #NOTE: need to fix validation because this val_generator only covers 1 file, not all 50\n",
    "                    \n",
    "        except(RuntimeError):\n",
    "            print('number of dims dont match in permute')\n",
    "            continue\n",
    "        file_num += 1 \n",
    "        epoch_acc = test_model(model, val_generator)\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        if len(losses) >= 50 and decrease == False:\n",
    "            last_loss = losses[-50]\n",
    "            if last_loss < running_avg_loss / len(train_generator):\n",
    "                decrease = True\n",
    "                trigger_times += 1\n",
    "                print('Trigger Times:', trigger_times)\n",
    "                if trigger_times >= patience:\n",
    "                    flag = True\n",
    "                    break\n",
    "            else:\n",
    "                trigger_times = 0\n",
    "        print(\"validation accuracy:\", epoch_acc)\n",
    "        scores.append(epoch_acc)\n",
    "        losses.append(running_avg_loss / len(train_generator))\n",
    "        print()\n",
    "        \n",
    "    save_checkpoint(model, optimizer, model_path, epoch)\n",
    "    if flag:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(50))[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "937a4285-061a-44cf-b15b-8284ea48b336",
     "kernelId": "4f378984-b18d-4458-aec5-11036b1449fe"
    }
   },
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print(f\"The model took {(end - start) // 60} minutes to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = preprocess(\"image_splits_v2/split_0.pkl\")\n",
    "tqdm(ex[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = dict(enumerate(k.flatten(), 1))\n",
    "# d = str(d) ## dump as string  (pickle and other packages parse the dump as bytes)\n",
    "\n",
    "m = ast.literal_eval(example1) ### convert the dict as str to  dict\n",
    "\n",
    "m = np.fromiter(m.values(), dtype=float) ## convert m to nparray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fromstring(example1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
